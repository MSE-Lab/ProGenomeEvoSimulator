#!/usr/bin/env python3
"""
Unified Evolution Engine
ç»Ÿä¸€çš„è¿›åŒ–å¼•æ“ - é›†æˆæ‰€æœ‰åŠŸèƒ½çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
åŒ…å«ï¼šä¼˜åŒ–ç®—æ³•ã€å¹¶è¡Œå¤„ç†ã€åŸºå› ä¸¢å¤±ã€ä¿å®ˆæ€§åˆ†æç­‰æ‰€æœ‰åŠŸèƒ½

Version: 1.0.0
Author: ProGenomeEvoSimulator Team
Date: 2025-09-27
"""

__version__ = "1.0.0"
__author__ = "ProGenomeEvoSimulator Team"
__date__ = "2025-09-27"

import time
import copy
import multiprocessing as mp
from multiprocessing import Pool, Manager
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
from functools import partial

from core.genome import Genome, Gene
from mechanisms.point_mutation_optimized import OptimizedPointMutationEngine
from mechanisms.horizontal_transfer import HorizontalGeneTransfer
from mechanisms.homologous_recombination import HomologousRecombination
from mechanisms.gene_loss import GeneLossEngine


class UnifiedEvolutionEngine:
    """
    ç»Ÿä¸€è¿›åŒ–å¼•æ“ - é›†æˆæ‰€æœ‰è¿›åŒ–æœºåˆ¶å’Œä¼˜åŒ–åŠŸèƒ½
    
    åŠŸèƒ½ç‰¹æ€§ï¼š
    - ç‚¹çªå˜ï¼ˆä¼˜åŒ–ç®—æ³•ï¼‰
    - æ¨ªå‘åŸºå› è½¬ç§»
    - åŒæºé‡ç»„
    - åŸºå› ä¸¢å¤±
    - å¹¶è¡Œå¤„ç†
    - æ€§èƒ½ä¼˜åŒ–
    - è¯¦ç»†ç»Ÿè®¡
    """
    
    def __init__(self, 
                 # åŸºæœ¬è¿›åŒ–å‚æ•°
                 mutation_rate: float = 1e-6,
                 hgt_rate: float = 1e-8,
                 recombination_rate: float = 1e-9,
                 min_similarity_for_recombination: float = 0.85,  # ä¿®æ­£ï¼šæ›´ä¸¥æ ¼çš„é‡ç»„ç›¸ä¼¼åº¦è¦æ±‚
                 
                 # åŸºå› ä¸¢å¤±å‚æ•°
                 enable_gene_loss: bool = True,
                 loss_rate: float = 1e-6,
                 core_gene_protection: float = 0.95,
                 hgt_gene_loss_multiplier: float = 10.0,
                 min_genome_size: int = 1000,
                 min_core_genes: int = 800,
                 optimal_genome_size: int = 3000,
                 
                 # å¹¶è¡Œå¤„ç†å‚æ•°
                 enable_parallel: bool = True,
                 num_processes: Optional[int] = None,
                 chunk_size: Optional[int] = None,
                 parallel_threshold: int = 500,  # åŸºå› æ•°è¶…è¿‡æ­¤å€¼æ—¶å¯ç”¨å¹¶è¡Œ
                 
                 # æ€§èƒ½ä¼˜åŒ–å‚æ•°
                 enable_optimization: bool = True,
                 enable_progress_sharing: bool = True):
        """
        åˆå§‹åŒ–ç»Ÿä¸€è¿›åŒ–å¼•æ“
        
        Args:
            # åŸºæœ¬è¿›åŒ–å‚æ•°
            mutation_rate: ç‚¹çªå˜ç‡
            hgt_rate: æ¨ªå‘åŸºå› è½¬ç§»ç‡
            recombination_rate: åŒæºé‡ç»„ç‡
            min_similarity_for_recombination: é‡ç»„æ‰€éœ€æœ€å°ç›¸ä¼¼åº¦
            
            # åŸºå› ä¸¢å¤±å‚æ•°
            enable_gene_loss: æ˜¯å¦å¯ç”¨åŸºå› ä¸¢å¤±
            loss_rate: åŸºå› ä¸¢å¤±ç‡
            core_gene_protection: æ ¸å¿ƒåŸºå› ä¿æŠ¤ç³»æ•°
            hgt_gene_loss_multiplier: HGTåŸºå› ä¸¢å¤±å€æ•°
            min_genome_size: æœ€å°åŸºå› ç»„å¤§å°
            min_core_genes: æœ€å°æ ¸å¿ƒåŸºå› æ•°
            optimal_genome_size: æœ€ä¼˜åŸºå› ç»„å¤§å°
            
            # å¹¶è¡Œå¤„ç†å‚æ•°
            enable_parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
            num_processes: å¹¶è¡Œè¿›ç¨‹æ•°
            chunk_size: åŸºå› åˆ†å—å¤§å°
            parallel_threshold: å¯ç”¨å¹¶è¡Œçš„åŸºå› æ•°é˜ˆå€¼
            
            # æ€§èƒ½ä¼˜åŒ–å‚æ•°
            enable_optimization: æ˜¯å¦å¯ç”¨æ€§èƒ½ä¼˜åŒ–
            enable_progress_sharing: æ˜¯å¦å¯ç”¨è¿›åº¦å…±äº«
        """
        
        # å­˜å‚¨é…ç½®å‚æ•°
        self.config = {
            'mutation_rate': mutation_rate,
            'hgt_rate': hgt_rate,
            'recombination_rate': recombination_rate,
            'min_similarity_for_recombination': min_similarity_for_recombination,
            'enable_gene_loss': enable_gene_loss,
            'loss_rate': loss_rate,
            'core_gene_protection': core_gene_protection,
            'hgt_gene_loss_multiplier': hgt_gene_loss_multiplier,
            'min_genome_size': min_genome_size,
            'min_core_genes': min_core_genes,
            'optimal_genome_size': optimal_genome_size,
            'enable_parallel': enable_parallel,
            'num_processes': num_processes or mp.cpu_count(),
            'chunk_size': chunk_size,
            'parallel_threshold': parallel_threshold,
            'enable_optimization': enable_optimization,
            'enable_progress_sharing': enable_progress_sharing
        }
        
        # åˆå§‹åŒ–è¿›åŒ–æœºåˆ¶
        self._initialize_mechanisms()
        
        # è¿›åŒ–å†å²è®°å½•
        self.evolution_history = []
        
        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
        self._print_initialization_info()
    
    def _initialize_mechanisms(self):
        """åˆå§‹åŒ–æ‰€æœ‰è¿›åŒ–æœºåˆ¶"""
        
        # 1. ç‚¹çªå˜å¼•æ“ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        if self.config['enable_optimization']:
            self.point_mutation = OptimizedPointMutationEngine(
                mutation_rate=self.config['mutation_rate'],
                enable_transition_bias=True,
                enable_hotspots=True
            )
        else:
            # å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ åŸºç¡€ç‰ˆæœ¬
            self.point_mutation = OptimizedPointMutationEngine(
                mutation_rate=self.config['mutation_rate']
            )
        
        # 2. æ¨ªå‘åŸºå› è½¬ç§»
        self.hgt = HorizontalGeneTransfer(self.config['hgt_rate'])
        
        # 3. åŒæºé‡ç»„ï¼ˆæ–°è®¾è®¡ï¼šå¤šç‚¹çªå˜æ¨¡å¼ï¼‰
        self.recombination = HomologousRecombination(
            recombination_rate=self.config['recombination_rate'],
            mutations_per_event=self.config.get('mutations_per_recombination_event', (5, 15)),
            enable_debug=self.config.get('recombination_debug', False)
        )
        
        # 4. åŸºå› ä¸¢å¤±ï¼ˆå¯é€‰ï¼‰
        if self.config['enable_gene_loss']:
            self.gene_loss = GeneLossEngine(
                loss_rate=self.config['loss_rate'],
                core_gene_protection=self.config['core_gene_protection'],
                hgt_gene_loss_multiplier=self.config['hgt_gene_loss_multiplier'],
                min_genome_size=self.config['min_genome_size'],
                min_core_genes=self.config['min_core_genes'],
                optimal_genome_size=self.config['optimal_genome_size']
            )
        else:
            self.gene_loss = None
    
    def _print_initialization_info(self):
        """æ‰“å°åˆå§‹åŒ–ä¿¡æ¯"""
        print(f"ğŸ§¬ Unified Evolution Engine initialized:")
        print(f"   Mechanisms: Point mutations, HGT, Recombination" + 
              (", Gene loss" if self.config['enable_gene_loss'] else ""))
        print(f"   Optimization: {'Enabled' if self.config['enable_optimization'] else 'Disabled'}")
        print(f"   Parallel processing: {'Enabled' if self.config['enable_parallel'] else 'Disabled'}")
        if self.config['enable_parallel']:
            print(f"   Processes: {self.config['num_processes']}")
            print(f"   Parallel threshold: {self.config['parallel_threshold']} genes")
        if self.config['enable_gene_loss']:
            print(f"   Gene loss rate: {self.config['loss_rate']}")
            print(f"   Core protection: {self.config['core_gene_protection']*100:.1f}%")
    
    def _should_use_parallel(self, genome: Genome) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¹¶è¡Œå¤„ç†"""
        return (self.config['enable_parallel'] and 
                genome.gene_count >= self.config['parallel_threshold'] and
                self.config['num_processes'] > 1)
    
    def _calculate_optimal_chunk_size(self, total_genes: int) -> int:
        """
        è®¡ç®—æœ€ä¼˜çš„åŸºå› åˆ†å—å¤§å°
        ä¼˜åŒ–ç­–ç•¥ï¼šå‡å°‘åˆ†å—æ•°é‡ï¼Œå¢åŠ æ¯å—å¤§å°ä»¥å‡å°‘é€šä¿¡å¼€é”€
        """
        if self.config['chunk_size']:
            return self.config['chunk_size']
        
        num_processes = self.config['num_processes']
        
        # ç›®æ ‡ï¼šæ¯ä¸ªè¿›ç¨‹å¤„ç†1-2ä¸ªå¤§å—ï¼Œè€Œä¸æ˜¯å¾ˆå¤šå°å—
        min_chunk_size = max(200, total_genes // (num_processes * 2))
        max_chunk_size = max(500, total_genes // num_processes)
        
        # ç¡®ä¿åˆ†å—å¤§å°åˆç†
        if total_genes < 1000:
            return max(100, total_genes // num_processes)
        else:
            return min(max_chunk_size, max(min_chunk_size, 300))
    
    def _split_genes_into_chunks(self, genes: List[Gene]) -> List[List[Gene]]:
        """å°†åŸºå› åˆ—è¡¨åˆ†å‰²æˆé€‚åˆå¹¶è¡Œå¤„ç†çš„å—"""
        chunk_size = self._calculate_optimal_chunk_size(len(genes))
        chunks = []
        
        for i in range(0, len(genes), chunk_size):
            chunk = genes[i:i + chunk_size]
            chunks.append(chunk)
        
        return chunks
    
    def evolve_one_generation_serial(self, genome: Genome) -> Dict:
        """ä¸²è¡Œè¿›åŒ–ä¸€ä»£"""
        generation_stats = {
            'generation': genome.generation + 1,
            'initial_stats': genome.get_statistics(),
            'mutations': 0,
            'hgt_events': 0,
            'recombination_events': 0,
            'genes_lost': 0,
            'processing_mode': 'serial'
        }
        
        # 1. åº”ç”¨ç‚¹çªå˜
        mutations = self.point_mutation.apply_mutations(genome, generations=1)
        generation_stats['mutations'] = mutations
        
        # 2. åº”ç”¨æ¨ªå‘åŸºå› è½¬ç§»
        hgt_events = self.hgt.apply_hgt(genome, generations=1)
        generation_stats['hgt_events'] = hgt_events
        
        # 3. åº”ç”¨åŒæºé‡ç»„
        recombination_events = self.recombination.apply_recombination(genome, generations=1)
        generation_stats['recombination_events'] = recombination_events
        
        # 4. åº”ç”¨åŸºå› ä¸¢å¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.gene_loss:
            genes_lost = self.gene_loss.apply_gene_loss(genome, generations=1)
            generation_stats['genes_lost'] = genes_lost
        
        # æ›´æ–°ä»£æ•°
        genome.generation += 1
        
        # è®°å½•æœ€ç»ˆç»Ÿè®¡
        generation_stats['final_stats'] = genome.get_statistics()
        
        return generation_stats
    
    def _get_or_create_process_pool(self):
        """è·å–æˆ–åˆ›å»ºè¿›ç¨‹æ± ï¼ˆé‡ç”¨ä»¥æé«˜æ€§èƒ½ï¼‰"""
        if not hasattr(self, '_process_pool') or self._process_pool is None:
            # å‡†å¤‡è¿›åŒ–å‚æ•°
            evolution_params = {
                'mutation_rate': self.config['mutation_rate'],
                'hgt_rate': self.config['hgt_rate'],
                'recombination_rate': self.config['recombination_rate'],
                'min_similarity_for_recombination': self.config['min_similarity_for_recombination'],
                'enable_transition_bias': True,
                'enable_hotspots': True
            }
            
            # åˆ›å»ºè¿›ç¨‹æ± å¹¶åˆå§‹åŒ–å·¥ä½œè¿›ç¨‹
            self._process_pool = Pool(
                processes=self.config['num_processes'],
                initializer=init_parallel_worker,
                initargs=(evolution_params,)
            )
        
        return self._process_pool
    
    def evolve_one_generation_parallel(self, genome: Genome) -> Dict:
        """ä¼˜åŒ–çš„å¹¶è¡Œè¿›åŒ–ä¸€ä»£"""
        generation_start_time = time.time()
        
        # ä½¿ç”¨ä¼˜åŒ–çš„åˆ†å—ç­–ç•¥
        gene_chunks = self._split_genes_into_chunks(genome.genes)
        
        generation_stats = {
            'generation': genome.generation + 1,
            'initial_stats': genome.get_statistics(),
            'total_mutations': 0,
            'total_hgt_events': 0,
            'total_recombination_events': 0,
            'genes_lost': 0,
            'chunks_processed': len(gene_chunks),
            'chunk_size': self._calculate_optimal_chunk_size(genome.gene_count),
            'parallel_processing_time': 0,
            'processing_mode': 'optimized_parallel'
        }
        
        parallel_start_time = time.time()
        
        # è·å–æˆ–åˆ›å»ºè¿›ç¨‹æ± 
        try:
            pool = self._get_or_create_process_pool()
            
            # å‡†å¤‡ä»»åŠ¡æ•°æ®ï¼ˆç§»é™¤å…±äº«è¿›åº¦ä»¥å‡å°‘é”ç«äº‰ï¼‰
            chunk_args = [(chunk, i) for i, chunk in enumerate(gene_chunks)]
            
            # å¹¶è¡Œæ‰§è¡Œ
            results = pool.map(evolve_genes_chunk_worker, chunk_args)
            
        except Exception as e:
            print(f"âŒ Parallel execution failed: {e}")
            # å›é€€åˆ°ä¸²è¡Œå¤„ç†
            return self.evolve_one_generation_serial(genome)
        
        parallel_end_time = time.time()
        generation_stats['parallel_processing_time'] = parallel_end_time - parallel_start_time
        
        # åˆå¹¶ç»“æœ
        evolved_genes = []
        for evolved_chunk, chunk_stats in results:
            evolved_genes.extend(evolved_chunk)
            generation_stats['total_mutations'] += chunk_stats['mutations']
            generation_stats['total_hgt_events'] += chunk_stats['hgt_events']
            generation_stats['total_recombination_events'] += chunk_stats['recombination_events']
        
        # ä¿®å¤ï¼šä¸ºäº†å…¼å®¹æ€§ï¼ŒåŒæ—¶è®¾ç½®ä¸¤ç§é”®å
        generation_stats['mutations'] = generation_stats['total_mutations']
        generation_stats['hgt_events'] = generation_stats['total_hgt_events']
        generation_stats['recombination_events'] = generation_stats['total_recombination_events']
        
        # æ›´æ–°åŸºå› ç»„
        genome.genes = evolved_genes
        genome.total_mutations += generation_stats['total_mutations']
        genome.total_hgt_events += generation_stats['total_hgt_events']
        genome.total_recombination_events += generation_stats['total_recombination_events']
        
        # åº”ç”¨åŸºå› ä¸¢å¤±ï¼ˆåœ¨ä¸»è¿›ç¨‹ä¸­å¤„ç†ï¼‰
        if self.gene_loss:
            genes_lost = self.gene_loss.apply_gene_loss(genome, generations=1)
            generation_stats['genes_lost'] = genes_lost
        
        # æ›´æ–°ä»£æ•°
        genome.generation += 1
        
        # è®°å½•æœ€ç»ˆç»Ÿè®¡
        generation_stats['final_stats'] = genome.get_statistics()
        generation_stats['total_processing_time'] = time.time() - generation_start_time
        
        return generation_stats
    
    def evolve_one_generation(self, genome: Genome) -> Dict:
        """è¿›åŒ–ä¸€ä»£ï¼ˆè‡ªåŠ¨é€‰æ‹©ä¸²è¡Œæˆ–å¹¶è¡Œï¼‰"""
        if self._should_use_parallel(genome):
            return self.evolve_one_generation_parallel(genome)
        else:
            return self.evolve_one_generation_serial(genome)
    
    def evolve_multiple_generations(self, genome: Genome, generations: int, 
                                  show_progress: bool = True) -> List[Dict]:
        """è¿›åŒ–å¤šä»£"""
        history = []
        start_time = time.time()
        
        # ç¡®å®šå¤„ç†æ¨¡å¼
        use_parallel = self._should_use_parallel(genome)
        processing_mode = "PARALLEL" if use_parallel else "SERIAL"
        
        if show_progress:
            # ç¡®å®šæ˜¾ç¤ºé¢‘ç‡
            if generations <= 50:
                display_freq = 5
            elif generations <= 100:
                display_freq = 10
            elif generations <= 1000:
                display_freq = 50
            else:
                display_freq = 100
            
            print(f"Starting {processing_mode} evolution simulation: {generations:,} generations")
            if use_parallel:
                print(f"Parallel processes: {self.config['num_processes']}")
            print(f"Progress updates every {display_freq} generation(s)")
            print("=" * 80)
        else:
            display_freq = max(1, generations // 10)
        
        for gen in range(generations):
            gen_start_time = time.time()
            
            # è¿›åŒ–ä¸€ä»£
            gen_stats = self.evolve_one_generation(genome)
            
            gen_end_time = time.time()
            gen_duration = gen_end_time - gen_start_time
            gen_stats['wall_clock_time'] = gen_duration
            
            history.append(gen_stats)
            
            # æ˜¾ç¤ºè¿›åº¦
            if show_progress and ((gen + 1) % display_freq == 0 or gen == 0):
                elapsed_total = time.time() - start_time
                avg_time_per_gen = elapsed_total / (gen + 1)
                remaining_gens = generations - (gen + 1)
                estimated_remaining = remaining_gens * avg_time_per_gen
                
                # è¿›åº¦æ¡
                progress = (gen + 1) / generations
                bar_width = 30
                filled_width = int(bar_width * progress)
                bar = 'â–ˆ' * filled_width + 'â–‘' * (bar_width - filled_width)
                
                # åŸºå› ç»„ä¿¡æ¯
                genome_info = (f"Genes: {genome.gene_count:,} | "
                             f"Events: {genome.total_mutations:,}mut "
                             f"{genome.total_hgt_events:,}HGT "
                             f"{genome.total_recombination_events:,}rec")
                
                # åŸºå› ä¸¢å¤±ä¿¡æ¯
                if self.gene_loss:
                    loss_stats = self.gene_loss.get_loss_statistics(genome)
                    loss_info = f" {loss_stats['total_genes_lost']:,}lost"
                    genome_info += loss_info
                
                # å¹¶è¡Œæ€§èƒ½ä¿¡æ¯
                if use_parallel and 'parallel_processing_time' in gen_stats:
                    parallel_efficiency = (gen_stats['parallel_processing_time'] / 
                                         gen_stats['total_processing_time']) * 100 if gen_stats['total_processing_time'] > 0 else 0
                    parallel_info = f" | Parallel: {parallel_efficiency:.1f}%"
                    genome_info += parallel_info
                
                print(f"\r[{bar}] {progress*100:.1f}% | Gen {gen + 1:,}/{generations:,} | "
                      f"{1/avg_time_per_gen:.1f} gen/s | ETA: {estimated_remaining/60:.1f}min | "
                      f"{genome_info}", end="", flush=True)
        
        if show_progress:
            total_time = time.time() - start_time
            print(f"\n\nğŸš€ {processing_mode} evolution completed!")
            print(f"Total time: {total_time/60:.2f} minutes ({total_time/3600:.2f} hours)")
            print(f"Average speed: {generations/total_time:.2f} generations/second")
            
            # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
            if self.gene_loss:
                loss_stats = self.gene_loss.get_loss_statistics(genome)
                print(f"Gene loss: {loss_stats['total_genes_lost']} genes lost "
                      f"({loss_stats['avg_total_loss_per_generation']:.3f}/gen)")
            
            if use_parallel:
                avg_parallel_efficiency = np.mean([
                    (h['parallel_processing_time'] / h['total_processing_time']) * 100 
                    for h in history if 'parallel_processing_time' in h and h['total_processing_time'] > 0
                ])
                print(f"Parallel efficiency: {avg_parallel_efficiency:.1f}%")
            
            print("=" * 80)
        
        self.evolution_history.extend(history)
        return history
    
    def simulate_evolution(self, 
                          initial_genome: Genome, 
                          generations: int,
                          save_snapshots: bool = True,
                          snapshot_interval: int = 100) -> Tuple[Genome, List[Dict]]:
        """å®Œæ•´çš„è¿›åŒ–æ¨¡æ‹Ÿ"""
        
        print("ğŸ§¬ UNIFIED PROKARYOTIC GENOME EVOLUTION SIMULATION")
        print("=" * 80)
        print(f"ğŸ“Š Initial genome: {initial_genome.gene_count:,} genes, {initial_genome.size:,} bp")
        print(f"ğŸ¯ Target generations: {generations:,}")
        print(f"ğŸ“¸ Snapshots: {'Enabled' if save_snapshots else 'Disabled'} (interval: {snapshot_interval})")
        
        # æ˜¾ç¤ºå¯ç”¨çš„åŠŸèƒ½
        features = []
        if self.config['enable_optimization']:
            features.append("Optimized algorithms")
        if self.config['enable_parallel']:
            features.append(f"Parallel processing ({self.config['num_processes']} cores)")
        if self.config['enable_gene_loss']:
            features.append("Gene loss")
        
        print(f"âš¡ Features: {', '.join(features)}")
        print(f"ğŸ§¬ Mechanisms: Point mutations, HGT, Homologous recombination" + 
              (", Gene loss" if self.config['enable_gene_loss'] else ""))
        print("=" * 80)
        
        # åˆ›å»ºåŸºå› ç»„å‰¯æœ¬
        evolving_genome = initial_genome.copy()
        simulation_start_time = time.time()
        
        # è®°å½•åˆå§‹çŠ¶æ€
        snapshots = []
        if save_snapshots:
            initial_summary = self.get_evolution_summary(evolving_genome)
            initial_summary['snapshot_generation'] = 0
            snapshots.append(initial_summary)
        
        # è¿›åŒ–è¿‡ç¨‹
        evolution_history = self.evolve_multiple_generations(
            evolving_genome, generations, show_progress=True
        )
        
        # ä¿å­˜å¿«ç…§
        if save_snapshots:
            print(f"ğŸ“¸ Saving snapshots every {snapshot_interval} generations...")
            for i in range(0, len(evolution_history), snapshot_interval):
                if i < len(evolution_history):
                    snapshot = self.get_evolution_summary(evolving_genome)
                    snapshot['snapshot_generation'] = evolution_history[i]['generation']
                    snapshots.append(snapshot)
        
        # æœ€ç»ˆæ€»ç»“
        total_simulation_time = time.time() - simulation_start_time
        
        print(f"\nğŸ‰ UNIFIED SIMULATION COMPLETED!")
        print(f"ğŸ§¬ Final genome: {evolving_genome.gene_count:,} genes, {evolving_genome.size:,} bp")
        print(f"ğŸ“ˆ Changes: {evolving_genome.size - initial_genome.size:+,} bp, "
              f"{evolving_genome.gene_count - initial_genome.gene_count:+,} genes")
        
        # æ€§èƒ½æ€»ç»“
        use_parallel = self._should_use_parallel(initial_genome)
        if use_parallel:
            print(f"âš¡ Performance: {self.config['num_processes']} processes, "
                  f"{generations/total_simulation_time:.2f} gen/s")
        else:
            print(f"âš¡ Performance: Serial processing, {generations/total_simulation_time:.2f} gen/s")
        
        return evolving_genome, snapshots
    
    def get_evolution_summary(self, genome: Genome) -> Dict:
        """è·å–è¿›åŒ–æ€»ç»“"""
        summary = {
            'genome_stats': genome.get_statistics(),
            'mutation_stats': self.point_mutation.get_mutation_statistics(genome),
            'hgt_stats': self.hgt.get_hgt_statistics(genome),
            'recombination_stats': self.recombination.get_recombination_statistics(genome),
            'engine_config': self.config.copy()
        }
        
        # æ·»åŠ åŸºå› ä¸¢å¤±ç»Ÿè®¡
        if self.gene_loss:
            summary['gene_loss_stats'] = self.gene_loss.get_loss_statistics(genome)
            summary['gene_loss_patterns'] = self.gene_loss.analyze_loss_patterns(genome)
        
        return summary
    
    def get_performance_analysis(self) -> Dict:
        """è·å–æ€§èƒ½åˆ†æ"""
        if not self.evolution_history:
            return {'error': 'No evolution history available'}
        
        analysis = {
            'total_generations': len(self.evolution_history),
            'processing_modes': {},
            'average_times': {},
            'engine_config': self.config.copy()
        }
        
        # åˆ†æå¤„ç†æ¨¡å¼
        serial_gens = [h for h in self.evolution_history if h.get('processing_mode') == 'serial']
        parallel_gens = [h for h in self.evolution_history if h.get('processing_mode') == 'parallel']
        
        analysis['processing_modes'] = {
            'serial_generations': len(serial_gens),
            'parallel_generations': len(parallel_gens)
        }
        
        # è®¡ç®—å¹³å‡æ—¶é—´
        if serial_gens:
            analysis['average_times']['serial'] = np.mean([h['wall_clock_time'] for h in serial_gens])
        
        if parallel_gens:
            analysis['average_times']['parallel'] = np.mean([h['wall_clock_time'] for h in parallel_gens])
            
            # å¹¶è¡Œæ•ˆç‡åˆ†æ
            parallel_efficiencies = []
            for h in parallel_gens:
                if 'parallel_processing_time' in h and h['total_processing_time'] > 0:
                    eff = (h['parallel_processing_time'] / h['total_processing_time']) * 100
                    parallel_efficiencies.append(eff)
            
            if parallel_efficiencies:
                analysis['parallel_efficiency'] = {
                    'average': np.mean(parallel_efficiencies),
                    'min': np.min(parallel_efficiencies),
                    'max': np.max(parallel_efficiencies)
                }
        
        return analysis
    
    def cleanup_parallel_resources(self):
        """æ¸…ç†å¹¶è¡Œèµ„æº"""
        if hasattr(self, '_process_pool') and self._process_pool is not None:
            self._process_pool.close()
            self._process_pool.join()
            self._process_pool = None
            print("ğŸ§¹ Parallel process pool cleaned up")
    
    def clear_caches(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        if hasattr(self.point_mutation, 'clear_cache'):
            self.point_mutation.clear_cache()
        self.cleanup_parallel_resources()
        print("ğŸ§¹ Caches cleared for memory optimization")
    
    def __del__(self):
        """ææ„å‡½æ•° - ç¡®ä¿èµ„æºæ¸…ç†"""
        try:
            self.cleanup_parallel_resources()
        except:
            pass  # å¿½ç•¥ææ„æ—¶çš„é”™è¯¯


# å…¨å±€å·¥ä½œè¿›ç¨‹çŠ¶æ€ - é¿å…é‡å¤åˆå§‹åŒ–
_worker_initialized = False
_worker_engines = None


def init_parallel_worker(evolution_params: Dict):
    """
    å·¥ä½œè¿›ç¨‹åˆå§‹åŒ–å‡½æ•° - åªåœ¨è¿›ç¨‹å¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡
    é¿å…æ¯æ¬¡ä»»åŠ¡éƒ½é‡æ–°åˆ›å»ºè¿›åŒ–æœºåˆ¶å®ä¾‹
    """
    global _worker_initialized, _worker_engines
    
    if _worker_initialized:
        return
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from mechanisms.point_mutation_optimized import OptimizedPointMutationEngine
        from mechanisms.horizontal_transfer import HorizontalGeneTransfer
        from mechanisms.homologous_recombination import HomologousRecombination
        
        # åˆ›å»ºè¿›åŒ–æœºåˆ¶å®ä¾‹ï¼ˆæ¯ä¸ªè¿›ç¨‹åªåˆ›å»ºä¸€æ¬¡ï¼‰
        point_mutation = OptimizedPointMutationEngine(
            mutation_rate=evolution_params['mutation_rate'],
            enable_transition_bias=evolution_params.get('enable_transition_bias', True),
            enable_hotspots=evolution_params.get('enable_hotspots', True)
        )
        
        hgt = HorizontalGeneTransfer(evolution_params['hgt_rate'])
        
        recombination = HomologousRecombination(
            recombination_rate=evolution_params['recombination_rate'],
            mutations_per_event=evolution_params.get('mutations_per_recombination_event', (5, 15)),
            enable_debug=False  # åœ¨å¹¶è¡Œæ¨¡å¼ä¸‹å…³é—­è°ƒè¯•ä»¥é¿å…è¾“å‡ºæ··ä¹±
        )
        
        _worker_engines = {
            'point_mutation': point_mutation,
            'hgt': hgt,
            'recombination': recombination
        }
        
        _worker_initialized = True
        
    except Exception as e:
        print(f"âŒ Worker initialization failed: {e}")
        _worker_initialized = False
        _worker_engines = None


def evolve_genes_chunk_worker(chunk_args: Tuple[List[Gene], int], 
                            evolution_params: Dict = None,
                            shared_progress: Optional[Any] = None) -> Tuple[List[Gene], Dict]:
    """
    ä¼˜åŒ–çš„å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼šå¤„ç†åŸºå› å—çš„è¿›åŒ–
    ä½¿ç”¨é¢„åˆå§‹åŒ–çš„è¿›åŒ–æœºåˆ¶å®ä¾‹ï¼Œå‡å°‘åˆ›å»ºå¼€é”€
    """
    global _worker_engines
    
    genes_chunk, process_id = chunk_args
    
    # æ£€æŸ¥å·¥ä½œè¿›ç¨‹æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
    if not _worker_engines:
        # å¦‚æœå·¥ä½œè¿›ç¨‹æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œè¿”å›åŸå§‹åŸºå› 
        return genes_chunk, {
            'process_id': process_id,
            'genes_processed': len(genes_chunk),
            'mutations': 0,
            'hgt_events': 0,
            'recombination_events': 0,
            'processing_time': 0,
            'error': 'Worker not initialized'
        }
    
    # ç»Ÿè®¡ä¿¡æ¯
    chunk_stats = {
        'process_id': process_id,
        'genes_processed': len(genes_chunk),
        'mutations': 0,
        'hgt_events': 0,
        'recombination_events': 0,
        'processing_time': 0
    }
    
    import time
    start_time = time.time()
    
    try:
        # åˆ›å»ºä¸´æ—¶åŸºå› ç»„ç”¨äºå¤„ç†è¿™ä¸ªå—
        from core.genome import Genome
        temp_genome = Genome(genes_chunk)
        
        # ä½¿ç”¨é¢„åˆå§‹åŒ–çš„è¿›åŒ–æœºåˆ¶
        engines = _worker_engines
        
        # åº”ç”¨è¿›åŒ–æœºåˆ¶
        mutations = engines['point_mutation'].apply_mutations(temp_genome, generations=1)
        chunk_stats['mutations'] = mutations
        
        hgt_events = engines['hgt'].apply_hgt(temp_genome, generations=1)
        chunk_stats['hgt_events'] = hgt_events
        
        recombination_events = engines['recombination'].apply_recombination(temp_genome, generations=1)
        chunk_stats['recombination_events'] = recombination_events
        
        # è¿”å›è¿›åŒ–åçš„åŸºå› 
        evolved_genes = temp_genome.genes
        
    except Exception as e:
        print(f"âŒ Error in process {process_id}: {e}")
        chunk_stats['error'] = str(e)
        evolved_genes = genes_chunk  # è¿”å›åŸå§‹åŸºå› 
    
    chunk_stats['processing_time'] = time.time() - start_time
    
    # ç§»é™¤å…±äº«è¿›åº¦æ›´æ–°ä»¥å‡å°‘é”ç«äº‰
    # if shared_progress is not None:
    #     try:
    #         with shared_progress.get_lock():
    #             shared_progress.value += len(genes_chunk)
    #     except:
    #         pass
    
    return evolved_genes, chunk_stats